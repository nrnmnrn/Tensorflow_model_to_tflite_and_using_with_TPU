{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================可用程式區【HEAD】=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 10:17:20.272762: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-30 10:17:20.273008: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbert\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtflite_runtime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreter\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtflite\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplatform\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#加速棒的code\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import bert\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import platform\n",
    "\n",
    "#加速棒的code\n",
    "EDGETPU_SHARED_LIB = {'Linux': 'libedgetpu.so.1' ,\n",
    "                      'Darwin': 'libedgetpu.1.dylib',\n",
    "                      'Windows': 'edgetpu.dll'}[platform.system()]\n",
    "def make_interpreter(model_file):\n",
    "    model_file , *device = model_file.split('@')\n",
    "    return tflite.Interpreter(model_path = model_file ,\n",
    "                              experimental_delegates = [tflite.load_delegate(EDGETPU_SHARED_LIB ,{'device': device[0]} if device else {})])\n",
    "\n",
    "class MobileBERT:\n",
    "\tdef __init__(self, tflite_path, tokenizer_file_path):\n",
    "\t\tself.max_length = 384 #要跟取用的模型吻合\n",
    "\t\tself.interpreter = make_interpreter(tflite_path)\n",
    "\t\tself.tokenizer = bert.bert_tokenization.FullTokenizer(tokenizer_file_path, True) #取用映射表作為tokenizer\n",
    "\t\tself.interpreter.allocate_tensors() #根據模型的要求，分配記憶體以供輸入和輸出張量使用。需要在執行推理之前，至少執行一次 allocate_tensors()。\n",
    "\t\tself.input_details = self.interpreter.get_input_details() #取得模型的input層資訊\n",
    "\t\tself.output_details = self.interpreter.get_output_details() #取得模型的output層資訊\n",
    "\n",
    "\tdef get_summary(self):\n",
    "\t\tprint(\"Inputs:\",self.input_details,\"\\nOutputs:\",self.output_details)\n",
    "\n",
    "\tdef get_masks(self,tokens):\n",
    "\t\tif len(tokens)>self.max_length:\n",
    "\t\t\traise IndexError(\"Token length more than max seq length!\")\n",
    "\t\treturn np.asarray([1]*len(tokens) + [0] * (self.max_length - len(tokens)))\n",
    "\n",
    "\n",
    "\tdef get_segments(self,tokens):\n",
    "\t\tif len(tokens)>self.max_length:\n",
    "\t\t\traise IndexError(\"Token length more than max seq length!\")\n",
    "\t\tsegments = []\n",
    "\t\tcurrent_segment_id = 0\n",
    "\t\tfor token in tokens:\n",
    "\t\t\tsegments.append(current_segment_id)\n",
    "\t\t\tif token == \"[SEP]\":\n",
    "\t\t\t\tcurrent_segment_id = 1\n",
    "\t\treturn np.asarray(segments + [0] * (self.max_length - len(tokens)))\n",
    "\n",
    "\n",
    "\tdef get_ids(self,tokens):\n",
    "\t\ttoken_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\t\tinput_ids = token_ids + [0] * (self.max_length-len(token_ids))\n",
    "\t\treturn np.asarray(input_ids)\n",
    "\n",
    "\tdef compile_text(self,text):\n",
    "\t\ttext = text.lower().replace(\"-\",\" \")\n",
    "\t\treturn [\"[CLS]\"] + self.tokenizer.tokenize(text) + [\"[SEP]\"]\n",
    "\n",
    "\tdef run(self,query,context):\n",
    "\t\tstokens =  self.compile_text(query) + self.compile_text(context)\n",
    "\n",
    "\t\tif len(stokens)>self.max_length:\n",
    "\t\t\traise IndexError(\"Token length more than max seq length!\")\n",
    "\t\t\tprint(\"Max exceeded\")\n",
    "\t\tinput_ids = tf.dtypes.cast(self.get_ids(stokens),tf.int32)\n",
    "\t\tinput_masks = tf.dtypes.cast(self.get_masks(stokens),tf.int32)\n",
    "\t\tinput_segments = tf.dtypes.cast(self.get_segments(stokens),tf.int32)\n",
    "\n",
    "\t\tself.interpreter.set_tensor(self.input_details[0]['index'], [input_ids])\n",
    "\t\tself.interpreter.set_tensor(self.input_details[1]['index'], [input_masks])\n",
    "\t\tself.interpreter.set_tensor(self.input_details[2]['index'], [input_segments])\n",
    "\n",
    "\t\twith tf.device('/CPU:0'):\n",
    "\t\t\tself.interpreter.invoke() #運行推理\n",
    "\n",
    "\t\tend_logits = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "\t\tstart_logits = self.interpreter.get_tensor(self.output_details[1]['index'])\n",
    "\n",
    "\t\tend = tf.argmax(end_logits,output_type=tf.dtypes.int32).numpy()[0]\n",
    "\t\tstart = tf.argmax(start_logits,output_type=tf.dtypes.int32).numpy()[0]\n",
    "\n",
    "\t\tanswers = \" \".join(stokens[start:end+1]).replace(\"[CLS]\",\"\").replace(\"[SEP]\",\"\").replace(\" ##\",\"\")\n",
    "\t\treturn answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***運作到這邊還沒東西就是真的沒答案啦***\n"
     ]
    }
   ],
   "source": [
    "m = MobileBERT('lite-model_mobilebert_1_metadata_1.tflite','vocab.txt') #tflite模型和tokenizer映射檔\n",
    "answer = m.run(\n",
    "\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by NASA, which succeeded in landing the first humans on the Moon from 1969 to 1972.\",\n",
    "\"What was the goal of the Apollo program?\"\n",
    ")\n",
    "print(answer)\n",
    "print(\"***運作到這邊還沒東西就是真的沒答案啦***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================可用程式區【END】=============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下為測試用程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrnmnrn/.pyenv/versions/3.8.5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-28 17:02:38.421574: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:02:38.422443: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-28 17:04:13.039430: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-28 17:04:13.147858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-28 17:04:17.347857: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-28 17:04:17.354670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\n",
      "2023-08-28 17:04:17.355698: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:04:17.356348: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:04:17.356905: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:04:17.357245: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:04:17.357644: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:04:17.357825: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:04:17.357993: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:04:17.358997: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:04:17.359104: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-28 17:04:17.435593: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-28 17:04:17.436105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-28 17:04:17.436154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
      "2023-08-28 17:04:18.093111: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForQuestionAnswering: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe0b664e6d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe0b63e4be0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe0b63faf70>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe0b638c640>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe0b76d2d90>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe0b76d5850>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fe121b434c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fe121b434c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, position_embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 420). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, position_embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjgilqc07/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjgilqc07/assets\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2023-08-28 17:09:17.875560: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-28 17:09:17.893253: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-08-28 17:09:17.919728: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-08-28 17:09:17.986759: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-28 17:09:17.994555: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-28 17:09:18.005190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\n",
      "2023-08-28 17:09:18.084228: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:09:18.085950: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:09:18.086170: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:09:18.088368: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:09:18.089216: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:09:18.089458: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:09:18.089763: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:09:18.090099: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-08-28 17:09:18.090226: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-28 17:09:19.396319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-28 17:09:19.396402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-08-28 17:09:19.398426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-08-28 17:09:19.542747: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1800005000 Hz\n",
      "2023-08-28 17:09:19.790000: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 1.017ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.008ms.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m無法執行程式碼，已處置工作階段。請嘗試重新啟動核心。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。請檢閱儲存格中的程式碼，找出失敗的可能原因。如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>，以取得進一步的詳細資料。"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "desired_model = \"distilbert-base-uncased\" #要調用的模型\n",
    "model = TFDistilBertForQuestionAnswering.from_pretrained(desired_model)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open('./mobileBERT2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2023-08-27 00:19:54.227539: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5a3e183520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5a3e183520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, position_embeddings_layer_call_fn while saving (showing 5 of 5315). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, position_embeddings_layer_call_fn while saving (showing 5 of 5315). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m無法執行程式碼，已處置工作階段。請嘗試重新啟動核心。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。請檢閱儲存格中的程式碼，找出失敗的可能原因。如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>，以取得進一步的詳細資料。"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open('./mobileBERT2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrnmnrn/.pyenv/versions/3.8.5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-27 00:15:39.901742: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:15:39.901888: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-27 00:16:03.534719: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-27 00:16:03.671505: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-27 00:16:05.482667: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-27 00:16:05.482945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\n",
      "2023-08-27 00:16:05.483354: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:16:05.483778: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:16:05.484058: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:16:05.484323: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:16:05.484582: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:16:05.484869: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:16:05.485085: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:16:05.485360: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-08-27 00:16:05.485409: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-27 00:16:05.497023: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-27 00:16:05.497612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-27 00:16:05.497692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
      "All model checkpoint layers were used when initializing TFMobileBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFMobileBertForQuestionAnswering were initialized from the model checkpoint at vumichien/mobilebert-uncased-squad-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a nice puppet'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFMobileBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "desired_model = \"vumichien/mobilebert-uncased-squad-v2\" #要調用的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(desired_model)\n",
    "model = TFMobileBertForQuestionAnswering.from_pretrained(desired_model)\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vumichien/mobilebert-uncased-squad-v2\")\n",
    "tflite_model_path = './lite-model_mobilebert_1_metadata_1.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "context = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by NASA, which succeeded in landing the first humans on the Moon from 1969 to 1972.\"\n",
    "question = \"What was the goal of the Apollo program?\"\n",
    "\n",
    "# 使用分詞器將文章和問題轉換成模型所需的輸入格式\n",
    "inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "\n",
    "input_ids  = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "input_mask = np.array(inputs['token_type_ids'], dtype=np.int32)\n",
    "segment_ids  = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "\n",
    "interpreter.set_tensor(input_details[0][\"index\"], input_ids)\n",
    "interpreter.set_tensor(input_details[1][\"index\"], input_mask)\n",
    "interpreter.set_tensor(input_details[2][\"index\"], segment_ids)\n",
    "interpreter.invoke()\n",
    "\n",
    "end_logits = interpreter.get_tensor(output_details[0][\"index\"])[0]\n",
    "start_logits = interpreter.get_tensor(output_details[1][\"index\"])[0]\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, start_logits : end_logits + 1]\n",
    "tokenizer.decode(predict_answer_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT\")\n",
    "tokenizer(question, context, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 51), dtype=int32, numpy=\n",
       "array([[  101,  2054,  2001,  1996,  3125,  1997,  1996,  9348,  2565,\n",
       "         1029,   102,  1996,  9348,  2565,  1010,  2036,  2124,  2004,\n",
       "         2622,  9348,  1010,  2001,  1996,  2353,  2142,  2163,  2529,\n",
       "         2686, 28968,  2565,  3344,  2041,  2011,  9274,  1010,  2029,\n",
       "         4594,  1999,  4899,  1996,  2034,  4286,  2006,  1996,  4231,\n",
       "         2013,  3440,  2000,  3285,  1012,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 51), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 51), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(question, context, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_text(text):\n",
    "\t\ttext = text.lower().replace(\"-\",\" \")\n",
    "\t\treturn [\"[CLS]\"] + tokenizer.tokenize(text) + [\"[SEP]\"]\n",
    "\n",
    "def run(query,context):\n",
    "\tinterpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "    input_details = self.input_details\n",
    "    output_details = self.output_details\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=np.int32)\n",
    "    input_mask = np.array(input_mask, dtype=np.int32)\n",
    "    segment_ids = np.array(segment_ids, dtype=np.int32)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], input_ids)\n",
    "    interpreter.set_tensor(input_details[1][\"index\"], input_mask)\n",
    "    interpreter.set_tensor(input_details[2][\"index\"], segment_ids)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    end_logits = interpreter.get_tensor(output_details[0][\"index\"])[0]\n",
    "    start_logits = interpreter.get_tensor(output_details[1][\"index\"])[0]\n",
    "    return start_logits, end_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 10:17:39.680853: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:39.681018: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/nrnmnrn/.pyenv/versions/3.8.5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-25 10:17:57.987749: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-25 10:17:58.010242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-25 10:17:59.701734: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-25 10:17:59.701829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\n",
      "2023-08-25 10:17:59.702146: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:59.702376: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:59.702551: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:59.703083: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:59.703293: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:59.703469: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:59.703872: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:59.704106: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-08-25 10:17:59.704141: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-25 10:17:59.708922: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-25 10:17:59.709059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-25 10:17:59.709080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
      "All model checkpoint layers were used when initializing TFMobileBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFMobileBertForQuestionAnswering were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat was the goal of the Apollo program?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m# 使用分詞器將文章和問題轉換成模型所需的輸入格式\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(question, text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m     17\u001b[0m answer_start_index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39margmax(outputs\u001b[39m.\u001b[39mstart_logits, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFMobileBertForQuestionAnswering, MobileBertTokenizer\n",
    "\n",
    "# 載入分詞器和模型\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "model = TFMobileBertForQuestionAnswering.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "# 輸入的文章和問題\n",
    "context = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by NASA, which succeeded in landing the first humans on the Moon from 1969 to 1972.\"\n",
    "question = \"What was the goal of the Apollo program?\"\n",
    "\n",
    "# 使用分詞器將文章和問題轉換成模型所需的輸入格式\n",
    "inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用分詞器將文章和問題轉換成模型所需的輸入格式\n",
    "inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open('./mobileBERT.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import MobileBertTokenizer\n",
    "import platform\n",
    "# import tflite_runtime.interpreter as tflite\n",
    "\n",
    "# tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "# tflite_model_path = \"./mobileBERT.tflite\"\n",
    "\n",
    "# EDGETPU_SHARED_LIB = {'Linux': 'libedgetpu.so.1' ,\n",
    "#                       'Darwin': 'libedgetpu.1.dylib',\n",
    "#                       'Windows': 'edgetpu.dll'}[platform.system()]\n",
    "\n",
    "# def make_interpreter(model_file):\n",
    "#     model_file , *device = model_file.split('@')\n",
    "#     return tflite.Interpreter(model_path = model_file ,\n",
    "#                               experimental_delegates = [tflite.load_delegate(EDGETPU_SHARED_LIB ,\n",
    "#                                                                              {'device': device[0]} if device else {})])\n",
    "# interpreter = make_interpreter(tflite_model_path)\n",
    "tflite_model_path = './lite-model_mobilebert_1_metadata_1.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Input context and question\n",
    "context = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by NASA, which succeeded in landing the first humans on the Moon from 1969 to 1972.\"\n",
    "question = \"What was the goal of the Apollo program?\"\n",
    "\n",
    "# Use tokenizer to convert context and question to model input format\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "\n",
    "input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "\n",
    "# input_ids = input_ids[:, :5]  \n",
    "\n",
    "# Run inference\n",
    "input_details = interpreter.get_input_details()\n",
    "interpreter.set_tensor(input_details[0]['index'], input_ids)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get prediction results\n",
    "start_logits = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n",
    "end_logits = interpreter.get_tensor(interpreter.get_output_details()[1]['index'])\n",
    "\n",
    "# Get the answer from the prediction\n",
    "start_index = np.argmax(start_logits)\n",
    "end_index = np.argmax(end_logits) + 1  # Ending index needs to be incremented by 1\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][start_index:end_index]))\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
